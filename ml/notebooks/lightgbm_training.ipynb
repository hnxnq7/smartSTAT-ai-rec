{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightGBM Demand Forecasting Training Pipeline\n",
        "\n",
        "This notebook trains and evaluates LightGBM models for demand forecasting on the smartSTAT medication datasets.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Set the data directory and other parameters at the top of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these paths as needed\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Get notebook directory and project root\n",
        "NOTEBOOK_DIR = Path().resolve()  # ml/notebooks/\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent  # project root\n",
        "\n",
        "# Add project root to path to import ml modules\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "# Data directory (update this if moving data to cloud storage)\n",
        "# Default: lib/smartstat_synth_2023-2025 relative to project root\n",
        "DATA_DIR = PROJECT_ROOT / \"lib\" / \"smartstat_synth_2023-2025\"\n",
        "\n",
        "# Output directory for models and results (relative to project root)\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"ml\" / \"models\"\n",
        "\n",
        "# Dataset ID to train (e.g., \"A1\", \"E10\", or use \"all\" to loop through all)\n",
        "DATASET_ID = \"A1\"\n",
        "\n",
        "# Forecast horizon (days): 7, 14, or 30\n",
        "HORIZON = 14\n",
        "\n",
        "# Model objective: \"l2\" (regression) or \"quantile\" (quantile regression)\n",
        "OBJECTIVE = \"l2\"\n",
        "\n",
        "# Quantile alpha (only used if OBJECTIVE == \"quantile\")\n",
        "QUANTILE_ALPHA = 0.95\n",
        "\n",
        "# Exclude contemporaneous used_units from features (use only lags)\n",
        "EXCLUDE_CONTEMPORANEOUS_USED = False\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Dataset ID: {DATASET_ID}\")\n",
        "print(f\"Horizon: {HORIZON} days\")\n",
        "print(f\"Objective: {OBJECTIVE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import json\n",
        "\n",
        "from ml.config import (\n",
        "    DEFAULT_LIGHTGBM_PARAMS,\n",
        "    VALIDATION_SPLIT_DAYS,\n",
        "    RANDOM_SEED,\n",
        "    EARLY_STOPPING_ROUNDS,\n",
        ")\n",
        "from ml.datasets import (\n",
        "    load_dataset_files,\n",
        "    get_all_dataset_ids,\n",
        "    get_dataset_info,\n",
        ")\n",
        "from ml.features import (\n",
        "    prepare_features_and_labels,\n",
        "    create_time_based_validation_split,\n",
        ")\n",
        "from ml.evaluate import (\n",
        "    compute_metrics,\n",
        "    save_predictions,\n",
        "    save_metrics,\n",
        ")\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"Imports complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "\n",
        "Load the training and test data for the selected dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get dataset info\n",
        "dataset_info = get_dataset_info(DATA_DIR, DATASET_ID)\n",
        "print(f\"Dataset Info:\")\n",
        "print(f\"  Type: {dataset_info['type']}\")\n",
        "print(f\"  Avg used (train): {dataset_info['avg_used_train']:.2f}\")\n",
        "print(f\"  Avg used (test): {dataset_info['avg_used_test']:.2f}\")\n",
        "print(f\"  Max used (train): {dataset_info['max_used_train']}\")\n",
        "print(f\"  Lead time: {dataset_info['lead_time_days']} days\")\n",
        "\n",
        "# Load datasets\n",
        "train_df, test_df = load_dataset_files(DATA_DIR, DATASET_ID)\n",
        "\n",
        "print(f\"\\nLoaded datasets:\")\n",
        "print(f\"  Train: {len(train_df)} rows ({train_df['date'].min()} to {train_df['date'].max()})\")\n",
        "print(f\"  Test: {len(test_df)} rows ({test_df['date'].min()} to {test_df['date'].max()})\")\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nTrain data preview:\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize usage patterns\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "# Training data\n",
        "axes[0].plot(train_df['date'], train_df['used_units'], alpha=0.7, linewidth=0.8)\n",
        "axes[0].set_title(f'Training Data: Daily Usage - Dataset {DATASET_ID}')\n",
        "axes[0].set_xlabel('Date')\n",
        "axes[0].set_ylabel('Used Units')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Test data\n",
        "axes[1].plot(test_df['date'], test_df['used_units'], alpha=0.7, linewidth=0.8, color='orange')\n",
        "axes[1].set_title(f'Test Data: Daily Usage - Dataset {DATASET_ID}')\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].set_ylabel('Used Units')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nUsage Statistics:\")\n",
        "print(f\"  Train - Mean: {train_df['used_units'].mean():.2f}, Std: {train_df['used_units'].std():.2f}\")\n",
        "print(f\"  Test  - Mean: {test_df['used_units'].mean():.2f}, Std: {test_df['used_units'].std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Features and Labels\n",
        "\n",
        "Create labels for multi-horizon forecasting and prepare feature sets. Labels are created using only future data (no leakage).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and labels\n",
        "X_train, y_train, X_test, y_test = prepare_features_and_labels(\n",
        "    train_df,\n",
        "    test_df,\n",
        "    horizon=HORIZON,\n",
        "    exclude_contemporaneous_used=EXCLUDE_CONTEMPORANEOUS_USED,\n",
        ")\n",
        "\n",
        "print(f\"Features ({len(X_train.columns)}):\")\n",
        "print(f\"  {', '.join(X_train.columns.tolist())}\")\n",
        "\n",
        "print(f\"\\nDataset sizes after label creation:\")\n",
        "print(f\"  Train: {len(X_train)} samples (last {HORIZON} rows dropped)\")\n",
        "print(f\"  Test: {len(X_test)} samples (last {HORIZON} rows dropped)\")\n",
        "\n",
        "print(f\"\\nLabel statistics (sum of next {HORIZON} days):\")\n",
        "print(f\"  Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}, Min: {y_train.min():.2f}, Max: {y_train.max():.2f}\")\n",
        "print(f\"  Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}, Min: {y_test.min():.2f}, Max: {y_test.max():.2f}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values in features:\")\n",
        "missing = X_train.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print(missing[missing > 0])\n",
        "else:\n",
        "    print(\"  None!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create time-based validation split\n",
        "X_train_split, y_train_split, X_val, y_val = create_time_based_validation_split(\n",
        "    X_train, y_train, validation_days=VALIDATION_SPLIT_DAYS\n",
        ")\n",
        "\n",
        "print(f\"Time-based validation split:\")\n",
        "print(f\"  Train split: {len(X_train_split)} samples\")\n",
        "print(f\"  Validation: {len(X_val)} samples (last {VALIDATION_SPLIT_DAYS} days)\")\n",
        "\n",
        "# Visualize label distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "axes[0].hist(y_train_split, bins=50, alpha=0.7, label='Train')\n",
        "axes[0].hist(y_val, bins=50, alpha=0.7, label='Validation')\n",
        "axes[0].set_xlabel(f'Label (sum of next {HORIZON} days)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Label Distribution: Train vs Validation')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(y_test, bins=50, alpha=0.7, color='orange')\n",
        "axes[1].set_xlabel(f'Label (sum of next {HORIZON} days)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Label Distribution: Test Set')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train LightGBM Model\n",
        "\n",
        "Train the model with early stopping on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure model parameters\n",
        "params = DEFAULT_LIGHTGBM_PARAMS.copy()\n",
        "\n",
        "if OBJECTIVE == \"quantile\":\n",
        "    params[\"objective\"] = \"quantile\"\n",
        "    params[\"alpha\"] = QUANTILE_ALPHA\n",
        "    print(f\"Using quantile regression with alpha={QUANTILE_ALPHA}\")\n",
        "else:\n",
        "    params[\"objective\"] = \"regression\"\n",
        "    params[\"metric\"] = \"rmse\"\n",
        "    print(f\"Using L2 regression (RMSE)\")\n",
        "\n",
        "print(f\"\\nModel parameters:\")\n",
        "for key, value in params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Create LightGBM datasets\n",
        "train_data = lgb.Dataset(X_train_split, label=y_train_split)\n",
        "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "print(f\"\\nTraining model...\")\n",
        "# Train model\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[train_data, val_data],\n",
        "    valid_names=[\"train\", \"val\"],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=True),\n",
        "        lgb.log_evaluation(period=50),\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "print(f\"Best iteration: {model.best_iteration}\")\n",
        "print(f\"Best score: {model.best_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "train_metric = list(model.best_score['train'].values())[0]\n",
        "val_metric = list(model.best_score['val'].values())[0]\n",
        "\n",
        "print(f\"Final metrics:\")\n",
        "print(f\"  Train {train_metric}: {model.best_score['train'][train_metric]:.4f}\")\n",
        "print(f\"  Val {val_metric}: {model.best_score['val'][val_metric]:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"feature\": X_train.columns,\n",
        "    \"importance\": model.feature_importance(importance_type=\"gain\"),\n",
        "}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 most important features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_n = min(15, len(feature_importance))\n",
        "sns.barplot(data=feature_importance.head(top_n), x=\"importance\", y=\"feature\", palette=\"viridis\")\n",
        "plt.title(f\"Feature Importance (Top {top_n})\")\n",
        "plt.xlabel(\"Importance (Gain)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on Test Set\n",
        "\n",
        "Make predictions on the test set and compute evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "\n",
        "# Compute metrics\n",
        "metrics = compute_metrics(y_test, pd.Series(y_pred))\n",
        "\n",
        "print(f\"Test Set Metrics:\")\n",
        "print(f\"  MAE: {metrics['mae']:.2f}\")\n",
        "print(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
        "if metrics['mape']:\n",
        "    print(f\"  MAPE: {metrics['mape']:.2f}% (calculated on {metrics['mape_count']}/{metrics['total_samples']} samples)\")\n",
        "else:\n",
        "    print(f\"  MAPE: N/A (insufficient non-zero targets)\")\n",
        "\n",
        "# Create predictions dataframe for visualization\n",
        "test_dates = test_df[\"date\"].iloc[:len(y_test)]\n",
        "predictions_df = pd.DataFrame({\n",
        "    \"date\": test_dates,\n",
        "    \"actual\": y_test.values,\n",
        "    \"predicted\": y_pred,\n",
        "    \"error\": y_test.values - y_pred,\n",
        "    \"abs_error\": np.abs(y_test.values - y_pred),\n",
        "})\n",
        "\n",
        "predictions_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actuals\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# Time series plot\n",
        "axes[0].plot(predictions_df['date'], predictions_df['actual'], \n",
        "             label='Actual', alpha=0.7, linewidth=1.5, color='blue')\n",
        "axes[0].plot(predictions_df['date'], predictions_df['predicted'], \n",
        "             label='Predicted', alpha=0.7, linewidth=1.5, color='red', linestyle='--')\n",
        "axes[0].set_xlabel('Date')\n",
        "axes[0].set_ylabel(f'Label (sum of next {HORIZON} days)')\n",
        "axes[0].set_title(f'Predictions vs Actuals - Dataset {DATASET_ID} (H={HORIZON})')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals plot\n",
        "axes[1].scatter(predictions_df['predicted'], predictions_df['error'], \n",
        "                alpha=0.5, s=20)\n",
        "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Residual (Actual - Predicted)')\n",
        "axes[1].set_title('Residual Plot')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot: predicted vs actual\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(predictions_df['actual'], predictions_df['predicted'], alpha=0.5, s=20)\n",
        "min_val = min(predictions_df['actual'].min(), predictions_df['predicted'].min())\n",
        "max_val = max(predictions_df['actual'].max(), predictions_df['predicted'].max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(f'Predicted vs Actual - Dataset {DATASET_ID} (H={HORIZON})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Error distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(predictions_df['error'], bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Prediction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Error Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model and Results\n",
        "\n",
        "Save the trained model, metrics, predictions, and feature importance to disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "model_dir = OUTPUT_DIR / DATASET_ID\n",
        "model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_filename = f\"model_h{HORIZON}_{OBJECTIVE}\"\n",
        "if OBJECTIVE == \"quantile\":\n",
        "    model_filename += f\"_q{QUANTILE_ALPHA}\"\n",
        "model_filename += \".txt\"\n",
        "model_path = model_dir / model_filename\n",
        "model.save_model(str(model_path))\n",
        "print(f\"✓ Saved model to: {model_path}\")\n",
        "\n",
        "# Save feature importance\n",
        "importance_path = model_dir / f\"feature_importance_h{HORIZON}_{OBJECTIVE}.csv\"\n",
        "feature_importance.to_csv(importance_path, index=False)\n",
        "print(f\"✓ Saved feature importance to: {importance_path}\")\n",
        "\n",
        "# Save predictions\n",
        "predictions_path = model_dir / f\"predictions_h{HORIZON}_{OBJECTIVE}.csv\"\n",
        "predictions_df.to_csv(predictions_path, index=False)\n",
        "print(f\"✓ Saved predictions to: {predictions_path}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics_path = model_dir / f\"metrics_h{HORIZON}_{OBJECTIVE}.json\"\n",
        "save_metrics(\n",
        "    metrics,\n",
        "    metrics_path,\n",
        "    DATASET_ID,\n",
        "    HORIZON,\n",
        "    OBJECTIVE,\n",
        "    model.params,\n",
        "    list(X_train.columns),\n",
        "    (str(train_df[\"date\"].min()), str(train_df[\"date\"].max())),\n",
        "    (str(test_df[\"date\"].min()), str(test_df[\"date\"].max())),\n",
        "    QUANTILE_ALPHA if OBJECTIVE == \"quantile\" else None,\n",
        ")\n",
        "print(f\"✓ Saved metrics to: {metrics_path}\")\n",
        "\n",
        "print(f\"\\nAll results saved to: {model_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Training and evaluation complete! The model has been saved along with all metrics and predictions.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try different horizons (7, 14, 30 days)\n",
        "- Experiment with quantile regression for uncertainty estimation\n",
        "- Train on multiple datasets using the CLI script: `python -m ml.train_lgbm --dataset_id all`\n",
        "- Adjust hyperparameters in the configuration section\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
